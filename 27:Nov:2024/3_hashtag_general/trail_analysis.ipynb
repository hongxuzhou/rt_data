{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 1] - Import required packages and setup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "# Configure logging to display in notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data with 50 items\n"
     ]
    }
   ],
   "source": [
    "# [Cell 2] - Load the data\n",
    "with open('/Users/hongxuzhou/rt_data/Related/cleaned_deathpositive_2024-11-27_05-42-41-750.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Successfully loaded data with {len(data)} items\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of first item:\n",
      "\n",
      "Main keys:\n",
      "- name\n",
      "- topPosts\n",
      "- related\n",
      "\n",
      "Component details:\n",
      "Related hashtags: 61 items\n",
      "Top posts: 31 posts\n"
     ]
    }
   ],
   "source": [
    "# [Cell 3] - Initial data inspection\n",
    "print(\"Structure of first item:\")\n",
    "first_item = data[0]\n",
    "print(\"\\nMain keys:\")\n",
    "for key in first_item.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Display the type and length of main components\n",
    "print(\"\\nComponent details:\")\n",
    "if 'related' in first_item:\n",
    "    print(f\"Related hashtags: {len(first_item['related'])} items\")\n",
    "if 'topPosts' in first_item:\n",
    "    print(f\"Top posts: {len(first_item['topPosts'])} posts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of related hashtags (first 5):\n",
      "#death: 11.9 m\n",
      "#cemetery: 3.29 m\n",
      "#graveyard: 2.35 m\n",
      "#macabre: 1.72 m\n",
      "#oddities: 1.56 m\n"
     ]
    }
   ],
   "source": [
    "# [Cell 4] - Examine related hashtags format\n",
    "print(\"\\nSample of related hashtags (first 5):\")\n",
    "if first_item.get('related'):\n",
    "    for hashtag_info in first_item['related'][:5]:\n",
    "        print(f\"#{hashtag_info['hash'][1:]}: {hashtag_info['info']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data completeness check:\n",
      "Items with related hashtags: 1/50\n",
      "Items with top posts: 49/50\n"
     ]
    }
   ],
   "source": [
    "# [Cell 5] - Data completeness check\n",
    "print(\"\\nData completeness check:\")\n",
    "has_related = sum(1 for item in data if 'related' in item and item['related'])\n",
    "has_topposts = sum(1 for item in data if 'topPosts' in item and item['topPosts'])\n",
    "\n",
    "print(f\"Items with related hashtags: {has_related}/{len(data)}\")\n",
    "print(f\"Items with top posts: {has_topposts}/{len(data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique number formats in 'info' field:\n",
      "Sample formats: ['129.17 k', '51.45 k', '32.48 k', '118.16 k', '60.15 k']\n"
     ]
    }
   ],
   "source": [
    "# [Cell 6] - Examine number formats in related hashtags\n",
    "print(\"\\nUnique number formats in 'info' field:\")\n",
    "if first_item.get('related'):\n",
    "    formats = set()\n",
    "    for item in first_item['related']:\n",
    "        formats.add(item['info'])\n",
    "    print(\"Sample formats:\", list(formats)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 7] - Define normalization functions\n",
    "def normalize_count(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert string counts like '129.17 k' to numerical values\n",
    "    Returns float or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = value.lower().strip()\n",
    "        if 'k' in value:\n",
    "            # Remove 'k' and convert to thousands\n",
    "            num = float(value.replace('k', '').strip())\n",
    "            return num * 1000\n",
    "        if 'm' in value:\n",
    "            # Remove 'm' and convert to millions\n",
    "            num = float(value.replace('m', '').strip())\n",
    "            return num * 1000000\n",
    "        return float(value)\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        logging.warning(f\"Could not normalize value: {value}, Error: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalization:\n",
      "Original: 129.17 k   -> Normalized: 129,170.00\n",
      "Original: 51.45 k    -> Normalized: 51,450.00\n",
      "Original: 11.9 m     -> Normalized: 11,900,000.00\n",
      "Original: 32.48 k    -> Normalized: 32,480.00\n",
      "Original: 118.16 k   -> Normalized: 118,160.00\n"
     ]
    }
   ],
   "source": [
    "# [Cell 8] - Test normalization function\n",
    "test_values = ['129.17 k', '51.45 k', '11.9 m', '32.48 k', '118.16 k']\n",
    "print(\"Testing normalization:\")\n",
    "for value in test_values:\n",
    "    normalized = normalize_count(value)\n",
    "    print(f\"Original: {value:10} -> Normalized: {normalized:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 9] - Create normalized hashtag dictionary\n",
    "def create_hashtag_dict(item: Dict) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Create dictionary of hashtag:count pairs from related field\n",
    "    \"\"\"\n",
    "    hashtag_counts = {}\n",
    "    if item.get('related'):\n",
    "        for hashtag_info in item['related']:\n",
    "            tag = hashtag_info['hash'].lstrip('#')  # Remove # if present\n",
    "            count = normalize_count(hashtag_info['info'])\n",
    "            if count is not None:\n",
    "                hashtag_counts[tag.lower()] = count\n",
    "    return hashtag_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized hashtag counts (first 5):\n",
      "#death: 11,900,000\n",
      "#cemetery: 3,290,000\n",
      "#graveyard: 2,350,000\n",
      "#macabre: 1,720,000\n",
      "#oddities: 1,560,000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [Cell 10] - Apply normalization to first item\n",
    "first_hashtag_counts = create_hashtag_dict(first_item)\n",
    "print(\"\\nNormalized hashtag counts (first 5):\")\n",
    "for tag, count in list(first_hashtag_counts.items())[:5]:\n",
    "    print(f\"#{tag}: {count:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 11] - Process full dataset\n",
    "def process_dataset(data: List[Dict]) -> List[Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Process entire dataset, creating normalized hashtag counts for each item\n",
    "    Returns list of processed items with hashtag counts\n",
    "    \"\"\"\n",
    "    processed_items = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Try related hashtags first\n",
    "        hashtag_counts = create_hashtag_dict(item)\n",
    "        \n",
    "        # If no related hashtags, log it\n",
    "        if not hashtag_counts:\n",
    "            logging.info(f\"No related hashtags found for item {len(processed_items)}\")\n",
    "            # Fallback mechanism would go here\n",
    "            \n",
    "        processed_items.append({\n",
    "            'main_hashtag': item.get('name', 'unknown'),\n",
    "            'hashtag_counts': hashtag_counts\n",
    "        })\n",
    "    \n",
    "    return processed_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No related hashtags found for item 1\n",
      "No related hashtags found for item 2\n",
      "No related hashtags found for item 3\n",
      "No related hashtags found for item 4\n",
      "No related hashtags found for item 5\n",
      "No related hashtags found for item 6\n",
      "No related hashtags found for item 7\n",
      "No related hashtags found for item 8\n",
      "No related hashtags found for item 9\n",
      "No related hashtags found for item 10\n",
      "No related hashtags found for item 11\n",
      "No related hashtags found for item 12\n",
      "No related hashtags found for item 13\n",
      "No related hashtags found for item 14\n",
      "No related hashtags found for item 15\n",
      "No related hashtags found for item 16\n",
      "No related hashtags found for item 17\n",
      "No related hashtags found for item 18\n",
      "No related hashtags found for item 19\n",
      "No related hashtags found for item 20\n",
      "No related hashtags found for item 21\n",
      "No related hashtags found for item 22\n",
      "No related hashtags found for item 23\n",
      "No related hashtags found for item 24\n",
      "No related hashtags found for item 25\n",
      "No related hashtags found for item 26\n",
      "No related hashtags found for item 27\n",
      "No related hashtags found for item 28\n",
      "No related hashtags found for item 29\n",
      "No related hashtags found for item 30\n",
      "No related hashtags found for item 31\n",
      "No related hashtags found for item 32\n",
      "No related hashtags found for item 33\n",
      "No related hashtags found for item 34\n",
      "No related hashtags found for item 35\n",
      "No related hashtags found for item 36\n",
      "No related hashtags found for item 37\n",
      "No related hashtags found for item 38\n",
      "No related hashtags found for item 39\n",
      "No related hashtags found for item 40\n",
      "No related hashtags found for item 41\n",
      "No related hashtags found for item 42\n",
      "No related hashtags found for item 43\n",
      "No related hashtags found for item 44\n",
      "No related hashtags found for item 45\n",
      "No related hashtags found for item 46\n",
      "No related hashtags found for item 47\n",
      "No related hashtags found for item 48\n",
      "No related hashtags found for item 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 50 items\n",
      "\n",
      "Sample of processed items:\n",
      "\n",
      "Main hashtag: deathpositive\n",
      "Number of related hashtags: 61\n",
      "Top 3 related hashtags by count:\n",
      "  #death: 11,900,000\n",
      "  #cemetery: 3,290,000\n",
      "  #graveyard: 2,350,000\n",
      "\n",
      "Main hashtag: deathpositivepup\n",
      "Number of related hashtags: 0\n"
     ]
    }
   ],
   "source": [
    "# [Cell 12] - Run processing and check results\n",
    "processed_data = process_dataset(data)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nProcessed {len(processed_data)} items\")\n",
    "print(\"\\nSample of processed items:\")\n",
    "for item in processed_data[:2]:  # Show first 2 items\n",
    "    print(f\"\\nMain hashtag: {item['main_hashtag']}\")\n",
    "    print(f\"Number of related hashtags: {len(item['hashtag_counts'])}\")\n",
    "    if item['hashtag_counts']:\n",
    "        print(\"Top 3 related hashtags by count:\")\n",
    "        top_3 = sorted(item['hashtag_counts'].items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)[:3]\n",
    "        for tag, count in top_3:\n",
    "            print(f\"  #{tag}: {count:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 13] - Implement fallback mechanism\n",
    "def extract_hashtags_from_posts(item: Dict) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract and count hashtags from topPosts as fallback\n",
    "    Returns dictionary of hashtag counts normalized to be comparable with 'related' data\n",
    "    \"\"\"\n",
    "    hashtag_counts = {}\n",
    "    \n",
    "    if not item.get('topPosts'):\n",
    "        logging.warning(\"No topPosts available for fallback counting\")\n",
    "        return hashtag_counts\n",
    "        \n",
    "    # Count hashtags from all posts\n",
    "    total_posts = len(item['topPosts'])\n",
    "    for post in item['topPosts']:\n",
    "        if post.get('hashtags'):\n",
    "            for tag in post['hashtags']:\n",
    "                tag = tag.lower()  # Normalize tag case\n",
    "                hashtag_counts[tag] = hashtag_counts.get(tag, 0) + 1\n",
    "    \n",
    "    # Remove the main hashtag if it exists in counts\n",
    "    main_tag = item.get('name', '').lower()\n",
    "    if main_tag in hashtag_counts:\n",
    "        del hashtag_counts[main_tag]\n",
    "    \n",
    "    return hashtag_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 14] - Updated processing function with fallback\n",
    "def process_dataset_with_fallback(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process dataset using related hashtags when available, falling back to topPosts when necessary\n",
    "    \"\"\"\n",
    "    processed_items = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Try related hashtags first\n",
    "        hashtag_counts = create_hashtag_dict(item)\n",
    "        \n",
    "        # Fallback to topPosts if no related hashtags\n",
    "        if not hashtag_counts:\n",
    "            logging.info(f\"Using fallback for item {len(processed_items)}\")\n",
    "            hashtag_counts = extract_hashtags_from_posts(item)\n",
    "        \n",
    "        processed_items.append({\n",
    "            'main_hashtag': item.get('name', 'unknown'),\n",
    "            'hashtag_counts': hashtag_counts,\n",
    "            'source': 'related' if create_hashtag_dict(item) else 'topPosts'\n",
    "        })\n",
    "    \n",
    "    return processed_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 14] - Updated processing function with fallback\n",
    "def process_dataset_with_fallback(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process dataset using related hashtags when available, falling back to topPosts when necessary\n",
    "    \"\"\"\n",
    "    processed_items = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Try related hashtags first\n",
    "        hashtag_counts = create_hashtag_dict(item)\n",
    "        \n",
    "        # Fallback to topPosts if no related hashtags\n",
    "        if not hashtag_counts:\n",
    "            logging.info(f\"Using fallback for item {len(processed_items)}\")\n",
    "            hashtag_counts = extract_hashtags_from_posts(item)\n",
    "        \n",
    "        processed_items.append({\n",
    "            'main_hashtag': item.get('name', 'unknown'),\n",
    "            'hashtag_counts': hashtag_counts,\n",
    "            'source': 'related' if create_hashtag_dict(item) else 'topPosts'\n",
    "        })\n",
    "    \n",
    "    return processed_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using fallback for item 1\n",
      "Using fallback for item 2\n",
      "Using fallback for item 3\n",
      "Using fallback for item 4\n",
      "Using fallback for item 5\n",
      "Using fallback for item 6\n",
      "Using fallback for item 7\n",
      "Using fallback for item 8\n",
      "Using fallback for item 9\n",
      "Using fallback for item 10\n",
      "Using fallback for item 11\n",
      "Using fallback for item 12\n",
      "Using fallback for item 13\n",
      "Using fallback for item 14\n",
      "Using fallback for item 15\n",
      "Using fallback for item 16\n",
      "Using fallback for item 17\n",
      "Using fallback for item 18\n",
      "Using fallback for item 19\n",
      "Using fallback for item 20\n",
      "Using fallback for item 21\n",
      "Using fallback for item 22\n",
      "Using fallback for item 23\n",
      "Using fallback for item 24\n",
      "Using fallback for item 25\n",
      "Using fallback for item 26\n",
      "Using fallback for item 27\n",
      "Using fallback for item 28\n",
      "Using fallback for item 29\n",
      "Using fallback for item 30\n",
      "Using fallback for item 31\n",
      "Using fallback for item 32\n",
      "Using fallback for item 33\n",
      "Using fallback for item 34\n",
      "Using fallback for item 35\n",
      "Using fallback for item 36\n",
      "Using fallback for item 37\n",
      "Using fallback for item 38\n",
      "Using fallback for item 39\n",
      "Using fallback for item 40\n",
      "No topPosts available for fallback counting\n",
      "Using fallback for item 41\n",
      "Using fallback for item 42\n",
      "Using fallback for item 43\n",
      "Using fallback for item 44\n",
      "Using fallback for item 45\n",
      "Using fallback for item 46\n",
      "Using fallback for item 47\n",
      "Using fallback for item 48\n",
      "Using fallback for item 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total items processed: 50\n",
      "Items using related data: 1\n",
      "Items using topPosts fallback: 49\n",
      "\n",
      "Sample results:\n",
      "\n",
      "Main hashtag: deathpositive\n",
      "Data source: related\n",
      "Number of related hashtags: 61\n",
      "Top 3 related hashtags:\n",
      "  #death: 11900000.0\n",
      "  #cemetery: 3290000.0\n",
      "  #graveyard: 2350000.0\n",
      "\n",
      "Main hashtag: deathpositivepup\n",
      "Data source: topPosts\n",
      "Number of related hashtags: 162\n",
      "Top 3 related hashtags:\n",
      "  #kermitthedog: 11\n",
      "  #deathpositive: 7\n",
      "  #grieftherapykermit: 6\n"
     ]
    }
   ],
   "source": [
    "# [Cell 15] - Test updated processing\n",
    "processed_data_with_fallback = process_dataset_with_fallback(data)\n",
    "\n",
    "# Display summary of results\n",
    "sources = {'related': 0, 'topPosts': 0}\n",
    "for item in processed_data_with_fallback:\n",
    "    sources[item['source']] += 1\n",
    "\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total items processed: {len(processed_data_with_fallback)}\")\n",
    "print(f\"Items using related data: {sources['related']}\")\n",
    "print(f\"Items using topPosts fallback: {sources['topPosts']}\")\n",
    "\n",
    "# Show sample results from each source\n",
    "print(\"\\nSample results:\")\n",
    "for item in processed_data_with_fallback[:2]:\n",
    "    print(f\"\\nMain hashtag: {item['main_hashtag']}\")\n",
    "    print(f\"Data source: {item['source']}\")\n",
    "    print(f\"Number of related hashtags: {len(item['hashtag_counts'])}\")\n",
    "    if item['hashtag_counts']:\n",
    "        print(\"Top 3 related hashtags:\")\n",
    "        top_3 = sorted(item['hashtag_counts'].items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)[:3]\n",
    "        for tag, count in top_3:\n",
    "            print(f\"  #{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 16] - Add normalization for comparing across sources\n",
    "def normalize_counts_for_comparison(hashtag_counts: Dict[str, float], \n",
    "                                 source: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Normalize counts to be comparable between sources\n",
    "    \"\"\"\n",
    "    if not hashtag_counts:\n",
    "        return {}\n",
    "        \n",
    "    if source == 'related':\n",
    "        # Already normalized by Instagram's counting\n",
    "        return hashtag_counts\n",
    "    elif source == 'topPosts':\n",
    "        # Convert to relative frequencies\n",
    "        total_hashtags = sum(hashtag_counts.values())\n",
    "        return {\n",
    "            tag: (count / total_hashtags) \n",
    "            for tag, count in hashtag_counts.items()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 17] - Update processing function\n",
    "def process_dataset_normalized(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process dataset with normalized counts for both sources\n",
    "    \"\"\"\n",
    "    processed_items = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Try related hashtags first\n",
    "        hashtag_counts = create_hashtag_dict(item)\n",
    "        source = 'related'\n",
    "        \n",
    "        # Fallback to topPosts if no related hashtags\n",
    "        if not hashtag_counts:\n",
    "            hashtag_counts = extract_hashtags_from_posts(item)\n",
    "            source = 'topPosts'\n",
    "        \n",
    "        # Normalize counts based on source\n",
    "        normalized_counts = normalize_counts_for_comparison(hashtag_counts, source)\n",
    "        \n",
    "        processed_items.append({\n",
    "            'main_hashtag': item.get('name', 'unknown'),\n",
    "            'hashtag_counts': normalized_counts,\n",
    "            'source': source,\n",
    "            'total_hashtags': len(normalized_counts)\n",
    "        })\n",
    "    \n",
    "    return processed_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No topPosts available for fallback counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample results with normalized counts:\n",
      "\n",
      "Main hashtag: deathpositive\n",
      "Data source: related\n",
      "Number of hashtags: 61\n",
      "Top 3 related hashtags:\n",
      "  #death: 11900000.000000\n",
      "  #cemetery: 3290000.000000\n",
      "  #graveyard: 2350000.000000\n",
      "\n",
      "Main hashtag: deathpositivepup\n",
      "Data source: topPosts\n",
      "Number of hashtags: 162\n",
      "Top 3 related hashtags:\n",
      "  #kermitthedog: 0.040892\n",
      "  #deathpositive: 0.026022\n",
      "  #grieftherapykermit: 0.022305\n"
     ]
    }
   ],
   "source": [
    "# [Cell 18] - Test normalized processing\n",
    "normalized_data = process_dataset_normalized(data)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results with normalized counts:\")\n",
    "for item in normalized_data[:2]:\n",
    "    print(f\"\\nMain hashtag: {item['main_hashtag']}\")\n",
    "    print(f\"Data source: {item['source']}\")\n",
    "    print(f\"Number of hashtags: {item['total_hashtags']}\")\n",
    "    if item['hashtag_counts']:\n",
    "        print(\"Top 3 related hashtags:\")\n",
    "        top_3 = sorted(item['hashtag_counts'].items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)[:3]\n",
    "        for tag, count in top_3:\n",
    "            print(f\"  #{tag}: {count:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No topPosts available for fallback counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample results with standardized frequencies:\n",
      "\n",
      "Main hashtag: deathpositive\n",
      "Data source: related\n",
      "Number of hashtags: 61\n",
      "Top 3 related hashtags:\n",
      "  #death: 0.346047\n",
      "  #cemetery: 0.095672\n",
      "  #graveyard: 0.068337\n",
      "\n",
      "Main hashtag: deathpositivepup\n",
      "Data source: topPosts\n",
      "Number of hashtags: 162\n",
      "Top 3 related hashtags:\n",
      "  #kermitthedog: 0.040892\n",
      "  #deathpositive: 0.026022\n",
      "  #grieftherapykermit: 0.022305\n",
      "\n",
      "Frequency Statistics:\n",
      "Mean frequency: 0.011551\n",
      "Median frequency: 0.004310\n",
      "Min frequency: 0.000880\n",
      "Max frequency: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# [Cell 19] - Standardize all measurements to relative frequencies\n",
    "def convert_to_relative_frequencies(hashtag_counts: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Convert any hashtag counts to relative frequencies\n",
    "    \"\"\"\n",
    "    if not hashtag_counts:\n",
    "        return {}\n",
    "    \n",
    "    # Calculate total for normalization\n",
    "    total = sum(hashtag_counts.values())\n",
    "    \n",
    "    # Convert to relative frequencies\n",
    "    return {\n",
    "        tag: count/total \n",
    "        for tag, count in hashtag_counts.items()\n",
    "    }\n",
    "\n",
    "def process_dataset_standardized(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process dataset using standardized relative frequencies for all items\n",
    "    \"\"\"\n",
    "    processed_items = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Try related hashtags first\n",
    "        hashtag_counts = create_hashtag_dict(item)\n",
    "        source = 'related'\n",
    "        \n",
    "        # Fallback to topPosts if no related hashtags\n",
    "        if not hashtag_counts:\n",
    "            hashtag_counts = extract_hashtags_from_posts(item)\n",
    "            source = 'topPosts'\n",
    "        \n",
    "        # Convert to relative frequencies regardless of source\n",
    "        freq_counts = convert_to_relative_frequencies(hashtag_counts)\n",
    "        \n",
    "        processed_items.append({\n",
    "            'main_hashtag': item.get('name', 'unknown'),\n",
    "            'hashtag_frequencies': freq_counts,\n",
    "            'source': source,\n",
    "            'total_hashtags': len(freq_counts)\n",
    "        })\n",
    "    \n",
    "    return processed_items\n",
    "\n",
    "# [Cell 20] - Test standardized processing\n",
    "standardized_data = process_dataset_standardized(data)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results with standardized frequencies:\")\n",
    "for item in standardized_data[:2]:\n",
    "    print(f\"\\nMain hashtag: {item['main_hashtag']}\")\n",
    "    print(f\"Data source: {item['source']}\")\n",
    "    print(f\"Number of hashtags: {item['total_hashtags']}\")\n",
    "    if item['hashtag_frequencies']:\n",
    "        print(\"Top 3 related hashtags:\")\n",
    "        top_3 = sorted(item['hashtag_frequencies'].items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)[:3]\n",
    "        for tag, freq in top_3:\n",
    "            print(f\"  #{tag}: {freq:.6f}\")\n",
    "\n",
    "# [Cell 21] - Basic statistics about frequencies\n",
    "print(\"\\nFrequency Statistics:\")\n",
    "all_frequencies = []\n",
    "for item in standardized_data:\n",
    "    if item['hashtag_frequencies']:\n",
    "        all_frequencies.extend(item['hashtag_frequencies'].values())\n",
    "\n",
    "if all_frequencies:\n",
    "    print(f\"Mean frequency: {np.mean(all_frequencies):.6f}\")\n",
    "    print(f\"Median frequency: {np.median(all_frequencies):.6f}\")\n",
    "    print(f\"Min frequency: {min(all_frequencies):.6f}\")\n",
    "    print(f\"Max frequency: {max(all_frequencies):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PMI scores...\n",
      "Total unique hashtags found: 3125\n",
      "Total occurrences: 49.00000000000082\n",
      "\n",
      "PMI Results Summary:\n",
      "Number of main hashtags with collocations: 50\n",
      "\n",
      "Sample of PMI scores:\n",
      "\n",
      "Main hashtag: #deathpositive\n",
      "Top 5 collocations:\n",
      "  #project_necropolis: 3.9420\n",
      "  #haunting: 3.9420\n",
      "  #graveyard_dead: 3.9420\n",
      "  #graveyard_freaks: 3.9420\n",
      "  #grave_affair: 3.9420\n",
      "\n",
      "Main hashtag: #deathpositivepup\n",
      "Top 5 collocations:\n",
      "  #theresalwaysdogs: 5.5245\n",
      "  #dogs: 5.5245\n",
      "  #dogsofinstagram: 5.5245\n",
      "  #doglover: 5.5245\n",
      "  #deathconcious: 5.5245\n"
     ]
    }
   ],
   "source": [
    "# [Cell 25] - Fixed PMI calculation with debugging\n",
    "def calculate_pmi_fixed(standardized_data: List[Dict]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate PMI with improved error handling and debugging\n",
    "    \"\"\"\n",
    "    # Calculate individual hashtag probabilities\n",
    "    total_occurrences = 0\n",
    "    hashtag_counts = {}\n",
    "    \n",
    "    # First pass: collect all hashtags and their frequencies\n",
    "    for item in standardized_data:\n",
    "        main_tag = item['main_hashtag'].lower()  # Normalize case\n",
    "        # Add main hashtag to counts\n",
    "        hashtag_counts[main_tag] = hashtag_counts.get(main_tag, 0) + 1\n",
    "        \n",
    "        # Add related hashtags to counts\n",
    "        for tag, freq in item['hashtag_frequencies'].items():\n",
    "            tag = tag.lower()  # Normalize case\n",
    "            hashtag_counts[tag] = hashtag_counts.get(tag, 0) + freq\n",
    "            total_occurrences += freq\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    hashtag_probs = {\n",
    "        tag: (count/total_occurrences) \n",
    "        for tag, count in hashtag_counts.items()\n",
    "    }\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"Total unique hashtags found: {len(hashtag_counts)}\")\n",
    "    print(f\"Total occurrences: {total_occurrences}\")\n",
    "    \n",
    "    # Calculate PMI scores\n",
    "    pmi_scores = {}\n",
    "    missed_tags = set()  # Track any tags we miss\n",
    "    \n",
    "    for item in standardized_data:\n",
    "        main_tag = item['main_hashtag'].lower()\n",
    "        if main_tag not in pmi_scores:\n",
    "            pmi_scores[main_tag] = {}\n",
    "        \n",
    "        for tag, freq in item['hashtag_frequencies'].items():\n",
    "            tag = tag.lower()\n",
    "            \n",
    "            # Skip self-collocations\n",
    "            if tag == main_tag:\n",
    "                continue\n",
    "            \n",
    "            # Verify we have probabilities for both tags\n",
    "            if main_tag not in hashtag_probs or tag not in hashtag_probs:\n",
    "                missed_tags.add(tag if tag not in hashtag_probs else main_tag)\n",
    "                continue\n",
    "            \n",
    "            # Calculate joint probability\n",
    "            joint_prob = freq / total_occurrences\n",
    "            \n",
    "            # Calculate PMI if we have non-zero probabilities\n",
    "            if joint_prob > 0 and hashtag_probs[main_tag] > 0 and hashtag_probs[tag] > 0:\n",
    "                pmi = np.log2(joint_prob / (hashtag_probs[main_tag] * hashtag_probs[tag]))\n",
    "                pmi_scores[main_tag][tag] = pmi\n",
    "    \n",
    "    if missed_tags:\n",
    "        print(f\"\\nWarning: Could not calculate PMI for {len(missed_tags)} tags\")\n",
    "        print(\"Sample of missed tags:\", list(missed_tags)[:5])\n",
    "    \n",
    "    return pmi_scores\n",
    "\n",
    "# [Cell 26] - Test fixed PMI calculation\n",
    "print(\"Calculating PMI scores...\")\n",
    "pmi_results = calculate_pmi_fixed(standardized_data)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPMI Results Summary:\")\n",
    "print(f\"Number of main hashtags with collocations: {len(pmi_results)}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(\"\\nSample of PMI scores:\")\n",
    "for main_tag, collocations in list(pmi_results.items())[:2]:  # Show first 2 items\n",
    "    if collocations:\n",
    "        print(f\"\\nMain hashtag: #{main_tag}\")\n",
    "        print(\"Top 5 collocations:\")\n",
    "        top_5 = sorted(collocations.items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)[:5]\n",
    "        for tag, pmi in top_5:\n",
    "            print(f\"  #{tag}: {pmi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adjusted collocation analysis...\n",
      "Collecting frequencies...\n",
      "\n",
      "Total occurrences: 49.0000\n",
      "Number of unique hashtags: 3124\n",
      "\n",
      "Filtering statistics:\n",
      "Total hashtag pairs examined: 4242\n",
      "Pairs filtered out (freq < 0.001): 4076\n",
      "Pairs retained: 166\n",
      "\n",
      "Detailed Collocation Analysis:\n",
      "\n",
      "Main hashtag: #deathpositive\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #macabre:\n",
      "    PMI: 3.2436\n",
      "    Count: 0.0500\n",
      "    Joint Probability: 0.0010\n",
      "    Tag Probability: 0.0017\n",
      "\n",
      "  #graveyard:\n",
      "    PMI: 2.9278\n",
      "    Count: 0.0683\n",
      "    Joint Probability: 0.0014\n",
      "    Tag Probability: 0.0028\n",
      "\n",
      "  #cemetery:\n",
      "    PMI: 2.6105\n",
      "    Count: 0.0957\n",
      "    Joint Probability: 0.0020\n",
      "    Tag Probability: 0.0049\n",
      "\n",
      "  #death:\n",
      "    PMI: 2.3281\n",
      "    Count: 0.3460\n",
      "    Joint Probability: 0.0071\n",
      "    Tag Probability: 0.0216\n"
     ]
    }
   ],
   "source": [
    "# [Cell 29] - Adjusted PMI analysis with debugging\n",
    "def analyze_collocations_debug(standardized_data: List[Dict], \n",
    "                             min_freq: float = 0.001) -> Dict[str, Dict[str, dict]]:\n",
    "    \"\"\"\n",
    "    Calculate PMI with debugging information and adjusted threshold\n",
    "    \"\"\"\n",
    "    # Initialize counts\n",
    "    pair_counts = {}\n",
    "    hashtag_counts = {}\n",
    "    total_occurrences = 0\n",
    "    \n",
    "    # First pass: collect frequencies\n",
    "    print(\"Collecting frequencies...\")\n",
    "    for item in standardized_data:\n",
    "        main_tag = item['main_hashtag'].lower()\n",
    "        if main_tag not in pair_counts:\n",
    "            pair_counts[main_tag] = {}\n",
    "            \n",
    "        # Count co-occurrences\n",
    "        for tag, freq in item['hashtag_frequencies'].items():\n",
    "            tag = tag.lower()\n",
    "            if tag == main_tag:\n",
    "                continue\n",
    "                \n",
    "            hashtag_counts[tag] = hashtag_counts.get(tag, 0) + freq\n",
    "            hashtag_counts[main_tag] = hashtag_counts.get(main_tag, 0) + freq\n",
    "            \n",
    "            if tag not in pair_counts[main_tag]:\n",
    "                pair_counts[main_tag][tag] = 0\n",
    "            pair_counts[main_tag][tag] += freq\n",
    "            total_occurrences += freq\n",
    "    \n",
    "    print(f\"\\nTotal occurrences: {total_occurrences:.4f}\")\n",
    "    print(f\"Number of unique hashtags: {len(hashtag_counts)}\")\n",
    "    \n",
    "    # Calculate enhanced statistics\n",
    "    results = {}\n",
    "    filtered_pairs = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for main_tag in pair_counts:\n",
    "        results[main_tag] = {}\n",
    "        \n",
    "        for tag, pair_count in pair_counts[main_tag].items():\n",
    "            total_pairs += 1\n",
    "            # Debug frequency filtering\n",
    "            if pair_count/total_occurrences < min_freq:\n",
    "                filtered_pairs += 1\n",
    "                continue\n",
    "                \n",
    "            # Calculate probabilities\n",
    "            p_xy = pair_count/total_occurrences\n",
    "            p_x = hashtag_counts[main_tag]/total_occurrences\n",
    "            p_y = hashtag_counts[tag]/total_occurrences\n",
    "            \n",
    "            # Calculate PMI\n",
    "            pmi = np.log2(p_xy/(p_x * p_y))\n",
    "            \n",
    "            results[main_tag][tag] = {\n",
    "                'pmi': pmi,\n",
    "                'count': pair_count,\n",
    "                'joint_prob': p_xy,\n",
    "                'tag_prob': p_y\n",
    "            }\n",
    "    \n",
    "    print(f\"\\nFiltering statistics:\")\n",
    "    print(f\"Total hashtag pairs examined: {total_pairs}\")\n",
    "    print(f\"Pairs filtered out (freq < {min_freq}): {filtered_pairs}\")\n",
    "    print(f\"Pairs retained: {total_pairs - filtered_pairs}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# [Cell 30] - Run adjusted analysis\n",
    "print(\"Running adjusted collocation analysis...\")\n",
    "collocation_results = analyze_collocations_debug(standardized_data, min_freq=0.001)\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nDetailed Collocation Analysis:\")\n",
    "for main_tag, collocations in list(collocation_results.items())[:2]:  # Show first 2\n",
    "    if not collocations:  # Skip if no collocations\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nMain hashtag: #{main_tag}\")\n",
    "    print(\"Top 5 collocations by PMI:\")\n",
    "    \n",
    "    # Sort by PMI score\n",
    "    sorted_collocations = sorted(\n",
    "        collocations.items(),\n",
    "        key=lambda x: x[1]['pmi'],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    for tag, stats in sorted_collocations:\n",
    "        print(f\"\\n  #{tag}:\")\n",
    "        print(f\"    PMI: {stats['pmi']:.4f}\")\n",
    "        print(f\"    Count: {stats['count']:.4f}\")\n",
    "        print(f\"    Joint Probability: {stats['joint_prob']:.4f}\")\n",
    "        print(f\"    Tag Probability: {stats['tag_prob']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete collocation analysis...\n",
      "Processing data...\n",
      "\n",
      "Data Summary:\n",
      "Total main hashtags processed: 50\n",
      "Total unique hashtags found: 3124\n",
      "Total occurrences: 49.0000\n",
      "\n",
      "Results by main hashtag:\n",
      "\n",
      "#deathpositivemi:\n",
      "  Pairs filtered: 18\n",
      "  Pairs retained: 3\n",
      "  Total pairs: 21\n",
      "\n",
      "#deathpositivehamilton:\n",
      "  Pairs filtered: 101\n",
      "  Pairs retained: 2\n",
      "  Total pairs: 103\n",
      "\n",
      "#deathpositiveartists:\n",
      "  Pairs filtered: 45\n",
      "  Pairs retained: 2\n",
      "  Total pairs: 47\n",
      "\n",
      "#deathpositivemoment:\n",
      "  Pairs filtered: 104\n",
      "  Pairs retained: 3\n",
      "  Total pairs: 107\n",
      "\n",
      "#deathpositiveuk:\n",
      "  Pairs filtered: 33\n",
      "  Pairs retained: 6\n",
      "  Total pairs: 39\n",
      "\n",
      "#deathpositivedog:\n",
      "  Pairs filtered: 25\n",
      "  Pairs retained: 3\n",
      "  Total pairs: 28\n",
      "\n",
      "#deathpositiveevents:\n",
      "  Pairs filtered: 29\n",
      "  Pairs retained: 4\n",
      "  Total pairs: 33\n",
      "\n",
      "#deathpositivejewelry:\n",
      "  Pairs filtered: 56\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 56\n",
      "\n",
      "#deathpositivecinema:\n",
      "  Pairs filtered: 27\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 27\n",
      "\n",
      "#deathpositivegifts:\n",
      "  Pairs filtered: 0\n",
      "  Pairs retained: 9\n",
      "  Total pairs: 9\n",
      "\n",
      "#deathpositivedocumentary:\n",
      "  Pairs filtered: 37\n",
      "  Pairs retained: 12\n",
      "  Total pairs: 49\n",
      "\n",
      "#deathpositivegrief:\n",
      "  Pairs filtered: 78\n",
      "  Pairs retained: 3\n",
      "  Total pairs: 81\n",
      "\n",
      "#deathpositivecommunity:\n",
      "  Pairs filtered: 209\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 209\n",
      "\n",
      "#deathpositivebarbie:\n",
      "  Pairs filtered: 58\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 58\n",
      "\n",
      "#deathpositiveadvocate:\n",
      "  Pairs filtered: 65\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 65\n",
      "\n",
      "#deathpositivemovement:\n",
      "  Pairs filtered: 244\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 244\n",
      "\n",
      "#deathpositiveau:\n",
      "  Pairs filtered: 25\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 25\n",
      "\n",
      "#deathpositivelibraries:\n",
      "  Pairs filtered: 160\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 161\n",
      "\n",
      "#deathpositiveplanner:\n",
      "  Pairs filtered: 25\n",
      "  Pairs retained: 5\n",
      "  Total pairs: 30\n",
      "\n",
      "#deathpositiveparenting:\n",
      "  Pairs filtered: 245\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 245\n",
      "\n",
      "#deathpositivereads:\n",
      "  Pairs filtered: 21\n",
      "  Pairs retained: 2\n",
      "  Total pairs: 23\n",
      "\n",
      "#deathpositivecat:\n",
      "  Pairs filtered: 0\n",
      "  Pairs retained: 16\n",
      "  Total pairs: 16\n",
      "\n",
      "#deathpositivepodcast:\n",
      "  Pairs filtered: 2\n",
      "  Pairs retained: 17\n",
      "  Total pairs: 19\n",
      "\n",
      "#deathpositivedc:\n",
      "  Pairs filtered: 142\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 143\n",
      "\n",
      "#deathpositivemarket:\n",
      "  Pairs filtered: 54\n",
      "  Pairs retained: 5\n",
      "  Total pairs: 59\n",
      "\n",
      "#deathpositiveitalia:\n",
      "  Pairs filtered: 41\n",
      "  Pairs retained: 2\n",
      "  Total pairs: 43\n",
      "\n",
      "#deathpositivelibrary:\n",
      "  Pairs filtered: 217\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 218\n",
      "\n",
      "#deathpositiveartist:\n",
      "  Pairs filtered: 293\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 293\n",
      "\n",
      "#deathpositive:\n",
      "  Pairs filtered: 57\n",
      "  Pairs retained: 4\n",
      "  Total pairs: 61\n",
      "\n",
      "#deathpositiveblog:\n",
      "  Pairs filtered: 29\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 29\n",
      "\n",
      "#deathpositivesundays:\n",
      "  Pairs filtered: 71\n",
      "  Pairs retained: 8\n",
      "  Total pairs: 79\n",
      "\n",
      "#deathpositivedoc:\n",
      "  Pairs filtered: 38\n",
      "  Pairs retained: 12\n",
      "  Total pairs: 50\n",
      "\n",
      "#deathpositivelibrarian:\n",
      "  Pairs filtered: 179\n",
      "  Pairs retained: 2\n",
      "  Total pairs: 181\n",
      "\n",
      "#deathpositiveevent:\n",
      "  Pairs filtered: 65\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 65\n",
      "\n",
      "#deathpositivewriting:\n",
      "  Pairs filtered: 14\n",
      "  Pairs retained: 9\n",
      "  Total pairs: 23\n",
      "\n",
      "#deathpositivetoronto:\n",
      "  Pairs filtered: 183\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 183\n",
      "\n",
      "#deathpositivepoetry:\n",
      "  Pairs filtered: 0\n",
      "  Pairs retained: 10\n",
      "  Total pairs: 10\n",
      "\n",
      "#deathpositivespaces:\n",
      "  Pairs filtered: 0\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 1\n",
      "\n",
      "#deathpositivekids:\n",
      "  Pairs filtered: 26\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 27\n",
      "\n",
      "#deathpositivelibraryservice:\n",
      "  Pairs filtered: 51\n",
      "  Pairs retained: 8\n",
      "  Total pairs: 59\n",
      "\n",
      "#deathpositivemovent:\n",
      "  Pairs filtered: 42\n",
      "  Pairs retained: 7\n",
      "  Total pairs: 49\n",
      "\n",
      "#deathpositivedesigns:\n",
      "  Pairs filtered: 177\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 177\n",
      "\n",
      "#deathpositiveoregon:\n",
      "  Pairs filtered: 52\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 52\n",
      "\n",
      "#deathpositiveart:\n",
      "  Pairs filtered: 228\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 228\n",
      "\n",
      "#deathpositiveculture:\n",
      "  Pairs filtered: 93\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 93\n",
      "\n",
      "#deathpositiveitaly:\n",
      "  Pairs filtered: 0\n",
      "  Pairs retained: 5\n",
      "  Total pairs: 5\n",
      "\n",
      "#deathpositivereading:\n",
      "  Pairs filtered: 34\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 35\n",
      "\n",
      "#deathpositivebooks:\n",
      "  Pairs filtered: 221\n",
      "  Pairs retained: 1\n",
      "  Total pairs: 222\n",
      "\n",
      "#deathpositivepup:\n",
      "  Pairs filtered: 162\n",
      "  Pairs retained: 0\n",
      "  Total pairs: 162\n",
      "\n",
      "Top Collocations for Each Hashtag:\n",
      "\n",
      "Main hashtag: #deathpositivemi\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #diebetter:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1481\n",
      "    Joint Probability: 0.0030\n",
      "\n",
      "  #deathdoulaintraining:\n",
      "    PMI: 5.5248\n",
      "    Count: 0.0741\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "  #thegooddeath:\n",
      "    PMI: 5.2380\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "Main hashtag: #deathpositivehamilton\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #hamont:\n",
      "    PMI: 4.9642\n",
      "    Count: 0.0658\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #deathpositivetoronto:\n",
      "    PMI: 1.2566\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositiveartists\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositiveart:\n",
      "    PMI: 1.1027\n",
      "    Count: 0.0517\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.3310\n",
      "    Count: 0.0517\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivemoment\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #letstalkdying:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0870\n",
      "    Joint Probability: 0.0018\n",
      "\n",
      "  #bringdeathtolife:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0815\n",
      "    Joint Probability: 0.0017\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: 0.5883\n",
      "    Count: 0.0978\n",
      "    Joint Probability: 0.0020\n",
      "\n",
      "Main hashtag: #deathpositiveuk\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #lossandgrief:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0633\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #sheffield:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0633\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #sheffielduk:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0506\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #sheffieldlifelossanddeathfestival:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0506\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #bereavementsupport:\n",
      "    PMI: 5.2523\n",
      "    Count: 0.0506\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "Main hashtag: #deathpositivedog\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #themodernmortician:\n",
      "    PMI: 5.2495\n",
      "    Count: 0.0645\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #kermitthedog:\n",
      "    PMI: 4.9065\n",
      "    Count: 0.0645\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #deathpositivepup:\n",
      "    PMI: 1.5703\n",
      "    Count: 0.0645\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "Main hashtag: #deathpositiveevents\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #greenburial:\n",
      "    PMI: 4.6942\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #naturalburial:\n",
      "    PMI: 3.8665\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #cemetery:\n",
      "    PMI: 3.4211\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: 0.2790\n",
      "    Count: 0.0789\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "Main hashtag: #deathpositivegifts\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #calavera:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "  #badassmom:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "  #snailmail:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "  #drawingreference:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "  #preciousstones:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.1111\n",
      "    Joint Probability: 0.0023\n",
      "\n",
      "Main hashtag: #deathpositivedocumentary\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #livewelldiebetter:\n",
      "    PMI: 3.9037\n",
      "    Count: 0.0492\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #exitstrategythemovie:\n",
      "    PMI: 3.9005\n",
      "    Count: 0.0511\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #fyp:\n",
      "    PMI: 3.9005\n",
      "    Count: 0.0511\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #deathdoulas:\n",
      "    PMI: 3.6273\n",
      "    Count: 0.0511\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #deathliteracy:\n",
      "    PMI: 3.2734\n",
      "    Count: 0.0530\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivegrief\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #mourningdoll:\n",
      "    PMI: 5.6111\n",
      "    Count: 0.0726\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "  #flomade:\n",
      "    PMI: 5.3401\n",
      "    Count: 0.1210\n",
      "    Joint Probability: 0.0025\n",
      "\n",
      "  #flomademourning:\n",
      "    PMI: 5.3129\n",
      "    Count: 0.0726\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "Main hashtag: #deathpositivelibraries\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.3144\n",
      "    Count: 0.0523\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositiveplanner\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #mementomorigoalplanner:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0893\n",
      "    Joint Probability: 0.0018\n",
      "\n",
      "  #etsyshop:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0714\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "  #printableplanner:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0714\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "  #deathpositivity:\n",
      "    PMI: 3.1650\n",
      "    Count: 0.0893\n",
      "    Joint Probability: 0.0018\n",
      "\n",
      "  #mementomori:\n",
      "    PMI: 2.9610\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivereads\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #dying:\n",
      "    PMI: 3.7681\n",
      "    Count: 0.0800\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "  #death:\n",
      "    PMI: 1.8837\n",
      "    Count: 0.0800\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "Main hashtag: #deathpositivecat\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #headswillroll:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #cathalloween:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #instacat:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #catsagram:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #catsofdayton:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "Main hashtag: #deathpositivepodcast\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #bestlifebestdeath:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0579\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #douladialogues:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0579\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #lifeanddeathtalks:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0579\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #endoflifeconversations:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0579\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #birthanddeathjourney:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0579\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "Main hashtag: #deathpositivedc\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: 0.4414\n",
      "    Count: 0.0884\n",
      "    Joint Probability: 0.0018\n",
      "\n",
      "Main hashtag: #deathpositivemarket\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #themortalsmarket:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0583\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #shopmortalsmarket:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0667\n",
      "    Joint Probability: 0.0014\n",
      "\n",
      "  #mortalsmarket:\n",
      "    PMI: 5.5780\n",
      "    Count: 0.1250\n",
      "    Joint Probability: 0.0026\n",
      "\n",
      "  #mortalsmarket2021:\n",
      "    PMI: 5.5724\n",
      "    Count: 0.1083\n",
      "    Joint Probability: 0.0022\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.3800\n",
      "    Count: 0.0500\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "Main hashtag: #deathpositiveitalia\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deatheducation:\n",
      "    PMI: 3.8274\n",
      "    Count: 0.0588\n",
      "    Joint Probability: 0.0012\n",
      "\n",
      "  #deathpositivity:\n",
      "    PMI: 2.9780\n",
      "    Count: 0.0784\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "Main hashtag: #deathpositivelibrary\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.5047\n",
      "    Count: 0.0514\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "Main hashtag: #deathpositive\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #macabre:\n",
      "    PMI: 3.2436\n",
      "    Count: 0.0500\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #graveyard:\n",
      "    PMI: 2.9278\n",
      "    Count: 0.0683\n",
      "    Joint Probability: 0.0014\n",
      "\n",
      "  #cemetery:\n",
      "    PMI: 2.6105\n",
      "    Count: 0.0957\n",
      "    Joint Probability: 0.0020\n",
      "\n",
      "  #death:\n",
      "    PMI: 2.3281\n",
      "    Count: 0.3460\n",
      "    Joint Probability: 0.0071\n",
      "\n",
      "Main hashtag: #deathpositivesundays\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #peppertooth:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0517\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #peppertoothsweeklyplanpartthree:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0647\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #musicmondays:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0647\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #readingtuesdays:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0647\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #hobbywednesdays:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0647\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "Main hashtag: #deathpositivedoc\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #livewelldiebetter:\n",
      "    PMI: 3.9554\n",
      "    Count: 0.0510\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #exitstrategythemovie:\n",
      "    PMI: 3.9467\n",
      "    Count: 0.0527\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #fyp:\n",
      "    PMI: 3.9467\n",
      "    Count: 0.0527\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #deathdoulas:\n",
      "    PMI: 3.6735\n",
      "    Count: 0.0527\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #deathliteracy:\n",
      "    PMI: 3.2671\n",
      "    Count: 0.0527\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivelibrarian\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #engaginglibs:\n",
      "    PMI: 4.3653\n",
      "    Count: 0.0509\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "  #deathpositivelibrary:\n",
      "    PMI: 1.1417\n",
      "    Count: 0.0509\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "Main hashtag: #deathpositivewriting\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #litmags:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0769\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "  #litmagsofinstagram:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0769\n",
      "    Joint Probability: 0.0016\n",
      "\n",
      "  #writing:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0641\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #litmag:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0897\n",
      "    Joint Probability: 0.0018\n",
      "\n",
      "  #writingcommunity:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0513\n",
      "    Joint Probability: 0.0010\n",
      "\n",
      "Main hashtag: #deathpositivepoetry\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpoem:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0909\n",
      "    Joint Probability: 0.0019\n",
      "\n",
      "  #deathpoetry:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0909\n",
      "    Joint Probability: 0.0019\n",
      "\n",
      "  #deathecology:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0909\n",
      "    Joint Probability: 0.0019\n",
      "\n",
      "  #treepoem:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0909\n",
      "    Joint Probability: 0.0019\n",
      "\n",
      "  #dailypoem:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0909\n",
      "    Joint Probability: 0.0019\n",
      "\n",
      "Main hashtag: #deathpositivespaces\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #mymysticaldeathbed:\n",
      "    PMI: 5.6147\n",
      "    Count: 1.0000\n",
      "    Joint Probability: 0.0204\n",
      "\n",
      "Main hashtag: #deathpositivekids\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: 0.6690\n",
      "    Count: 0.1034\n",
      "    Joint Probability: 0.0021\n",
      "\n",
      "Main hashtag: #deathpositivelibraryservice\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #whatsonfarnham:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #whatsonsurrey:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #farnhamactivities:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #surreyactivities:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #farnham:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0536\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivemovent\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #mort:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "  #obsques:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0658\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #enterrement:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.0724\n",
      "    Joint Probability: 0.0015\n",
      "\n",
      "  #deuil:\n",
      "    PMI: 5.5614\n",
      "    Count: 0.0658\n",
      "    Joint Probability: 0.0013\n",
      "\n",
      "  #funeraire:\n",
      "    PMI: 5.5484\n",
      "    Count: 0.0526\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositiveitaly\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #elenapradella:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.2000\n",
      "    Joint Probability: 0.0041\n",
      "\n",
      "  #tanexpo2021:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.2000\n",
      "    Joint Probability: 0.0041\n",
      "\n",
      "  #ioscattoatanexpo:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.2000\n",
      "    Joint Probability: 0.0041\n",
      "\n",
      "  #shareyourtanexpo:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.2000\n",
      "    Joint Probability: 0.0041\n",
      "\n",
      "  #preplanfuneralservice:\n",
      "    PMI: 5.6147\n",
      "    Count: 0.2000\n",
      "    Joint Probability: 0.0041\n",
      "\n",
      "Main hashtag: #deathpositivereading\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.2348\n",
      "    Count: 0.0556\n",
      "    Joint Probability: 0.0011\n",
      "\n",
      "Main hashtag: #deathpositivebooks\n",
      "Top 5 collocations by PMI:\n",
      "\n",
      "  #deathpositive:\n",
      "    PMI: -0.0515\n",
      "    Count: 0.0673\n",
      "    Joint Probability: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# [Cell 31] - Complete PMI analysis for all hashtags\n",
    "def analyze_all_collocations(standardized_data: List[Dict], \n",
    "                           min_freq: float = 0.001) -> Dict[str, Dict[str, dict]]:\n",
    "    \"\"\"\n",
    "    Calculate PMI for all hashtags with improved tracking\n",
    "    \"\"\"\n",
    "    # Initialize counts\n",
    "    pair_counts = {}\n",
    "    hashtag_counts = {}\n",
    "    total_occurrences = 0\n",
    "    processed_tags = set()\n",
    "    \n",
    "    print(\"Processing data...\")\n",
    "    # First pass: collect all hashtags and frequencies\n",
    "    for item in standardized_data:\n",
    "        main_tag = item['main_hashtag'].lower()\n",
    "        processed_tags.add(main_tag)\n",
    "        \n",
    "        if main_tag not in pair_counts:\n",
    "            pair_counts[main_tag] = {}\n",
    "            \n",
    "        # Count co-occurrences\n",
    "        for tag, freq in item['hashtag_frequencies'].items():\n",
    "            tag = tag.lower()\n",
    "            if tag == main_tag:\n",
    "                continue\n",
    "                \n",
    "            hashtag_counts[tag] = hashtag_counts.get(tag, 0) + freq\n",
    "            hashtag_counts[main_tag] = hashtag_counts.get(main_tag, 0) + freq\n",
    "            \n",
    "            if tag not in pair_counts[main_tag]:\n",
    "                pair_counts[main_tag][tag] = 0\n",
    "            pair_counts[main_tag][tag] += freq\n",
    "            total_occurrences += freq\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"Total main hashtags processed: {len(processed_tags)}\")\n",
    "    print(f\"Total unique hashtags found: {len(hashtag_counts)}\")\n",
    "    print(f\"Total occurrences: {total_occurrences:.4f}\")\n",
    "    \n",
    "    # Calculate PMI for each main hashtag\n",
    "    results = {}\n",
    "    filtered_counts = {tag: 0 for tag in processed_tags}\n",
    "    retained_counts = {tag: 0 for tag in processed_tags}\n",
    "    \n",
    "    for main_tag in processed_tags:\n",
    "        results[main_tag] = {}\n",
    "        \n",
    "        if main_tag not in pair_counts:\n",
    "            print(f\"Warning: No pairs found for #{main_tag}\")\n",
    "            continue\n",
    "            \n",
    "        for tag, pair_count in pair_counts[main_tag].items():\n",
    "            # Apply frequency filter\n",
    "            if pair_count/total_occurrences < min_freq:\n",
    "                filtered_counts[main_tag] += 1\n",
    "                continue\n",
    "                \n",
    "            # Calculate probabilities\n",
    "            p_xy = pair_count/total_occurrences\n",
    "            p_x = hashtag_counts[main_tag]/total_occurrences\n",
    "            p_y = hashtag_counts[tag]/total_occurrences\n",
    "            \n",
    "            # Calculate PMI\n",
    "            pmi = np.log2(p_xy/(p_x * p_y))\n",
    "            \n",
    "            results[main_tag][tag] = {\n",
    "                'pmi': pmi,\n",
    "                'count': pair_count,\n",
    "                'joint_prob': p_xy,\n",
    "                'tag_prob': p_y\n",
    "            }\n",
    "            retained_counts[main_tag] += 1\n",
    "    \n",
    "    # Display results for each main hashtag\n",
    "    print(\"\\nResults by main hashtag:\")\n",
    "    for tag in processed_tags:\n",
    "        total = filtered_counts[tag] + retained_counts[tag]\n",
    "        if total > 0:\n",
    "            print(f\"\\n#{tag}:\")\n",
    "            print(f\"  Pairs filtered: {filtered_counts[tag]}\")\n",
    "            print(f\"  Pairs retained: {retained_counts[tag]}\")\n",
    "            print(f\"  Total pairs: {total}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# [Cell 32] - Run complete analysis\n",
    "print(\"Running complete collocation analysis...\")\n",
    "full_results = analyze_all_collocations(standardized_data)\n",
    "\n",
    "# Display top collocations for each hashtag\n",
    "print(\"\\nTop Collocations for Each Hashtag:\")\n",
    "for main_tag, collocations in full_results.items():\n",
    "    if not collocations:  # Skip if no collocations\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nMain hashtag: #{main_tag}\")\n",
    "    print(\"Top 5 collocations by PMI:\")\n",
    "    \n",
    "    # Sort by PMI score\n",
    "    sorted_collocations = sorted(\n",
    "        collocations.items(),\n",
    "        key=lambda x: x[1]['pmi'],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    for tag, stats in sorted_collocations:\n",
    "        print(f\"\\n  #{tag}:\")\n",
    "        print(f\"    PMI: {stats['pmi']:.4f}\")\n",
    "        print(f\"    Count: {stats['count']:.4f}\")\n",
    "        print(f\"    Joint Probability: {stats['joint_prob']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
